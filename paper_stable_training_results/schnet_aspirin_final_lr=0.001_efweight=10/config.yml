# 基础硬件设置
amp: false  # 混合精度训练，32 位浮点数，精度更高但显存占用更大
checkpoint: null  # 从头训练，不加载之前的断点
cpu: false  # 不使用 CPU，强制使用 GPU

# 数据集设置
dataset:
- grad_target_mean: 0.0  # grad target--能量梯度--力 力的预测目标的平均值（平衡平均受力为0）
  grad_target_std: 1.0  # 力的（梯度）归一化参数，均值为 0，方差为 1，帮助模型收敛  均值=0，标准差=1，∴力保持原始数值
  molecule: aspirin  # 训练的分子对象
  name: md17-aspirin  # 任务名称，通常用于生成日志文件夹
  normalize_labels: true  # 是否对标签（能量/力）进行归一化（平均）处理，有助于模型更快收敛
  size: 1k  # 使用的数据集样本量大小为 1000 个结构
  src: post_data/md17/aspirin/1k/train  # 训练数据在服务器上的路径
  target_mean: -17637.827343506295  # 能量的统计平均值。由于原子能量数值极大，训练前需减去此均值
  target_std: 1.0  # 能量的标准差  Z=(样本值-平均值)/标准差   标准差设置为1，可能波动范围并不大因此没有进行缩小 ∴数据分布中心化
- src: post_data/md17/aspirin/1k/val
distributed: false  # 是否开启多显卡分布式训练
distributed_backend: nccl  # 多卡通信使用的后端驱动（NVIDIA 显卡常用 nccl）
distributed_port: 13356  # 多卡通信使用的端口号
identifier: ''
is_debug: false
local_rank: 0
logger:
  name: wandb
  project: mdbench
mode: train

# 模型架构
model:
  cutoff: 5.0  # 截断半径
  hidden_channels: 64  # 隐藏层通道数，通道数↑，信息储存量↑ ————神经网络宽度（广度）
  name: schnet   # 模型架构名称
  num_filters: 64
  num_gaussians: 25
  num_interactions: 6  # 交互层数————神经网络深度
  otf_graph: true
  use_pbc: false
noddp: false

# 优化器
optim:
  batch_size: 100  # 一次训练读取 100 个分子结构
  checkpoint_every: 10
  early_stopping_lr: 1.0e-06
  early_stopping_time: 604800
  energy_coefficient: 0.05   # loss(QM)中能量的权重
  eval_batch_size: 100
  factor: 0.8
  force_coefficient: 0.95  # # loss(QM)中力的权重
  lr_gamma: 0.1
  lr_initial: 0.001  # 初始学习率
  max_epochs: 10000  # 最多跑 1 万轮
  min_lr: 1.0e-06
  num_workers: 8
  optimizer: Adam
  patience: 5
  scheduler: ReduceLROnPlateau  # Loss不下降，自动降低学习率
  warmup_factor: 0.2
  warmup_steps: 5000
print_every: 200
run_dir: MODELPATH/
seed: 0
submit: false
summit: false
task:
  dataset: lmdb
  description: Regressing to energies and forces
  eval_on_free_atoms: true
  grad_input: atomic forces
  labels:
  - potential energy
  metric: mae
  train_on_free_atoms: true
  type: regression
timestamp_id: null
trainer: trainer
world_size: 1
